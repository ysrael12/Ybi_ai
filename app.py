import os
import streamlit as st
import pandas as pd
import random
import gc
import time

# --- 0. SETUP DE SEGURAN√áA ---
# Configura√ß√µes para tentar economizar cada MB de RAM
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:32"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# Defini√ß√£o de chaves dummy para o Mangaba n√£o travar
if "GOOGLE_API_KEY" not in os.environ:
    os.environ["GOOGLE_API_KEY"] = "AIzaSy_CHAVE_DUMMY_PARA_EVITAR_ERRO"
    os.environ["LLM_PROVIDER"] = "google"

# Imports protegidos
try:
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from peft import PeftModel
    from mangaba import Agent
except ImportError as e:
    st.error(f"Erro de depend√™ncia: {e}")
    st.stop()

# --- CONFIGURA√á√ÉO DA P√ÅGINA ---
st.set_page_config(
    page_title="YBY.AI - Monitoramento Agro",
    page_icon="üå±",
    layout="wide"
)

# --- 1. CARREGAMENTO DO MODELO COM PROTE√á√ÉO DE MEM√ìRIA ---
@st.cache_resource(show_spinner=False)
def load_engine_safely():
    """
    Tenta carregar a IA. Se a mem√≥ria explodir, retorna None (Modo Demo).
    """
    container = st.empty()
    container.info("‚öôÔ∏è Iniciando Motor de IA... (Monitorando Mem√≥ria)")
    
    BASE_MODEL = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    ADAPTER_REPO = "YsraelJS/tinyllama-solo-management-adapters"

    try:
        # Coleta de lixo for√ßada antes de come√ßar
        gc.collect()
        
        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
        
        # Tenta carregar apenas a estrutura (sem pesos pesados ainda)
        # Se estiver no Streamlit Cloud, isso √© arriscado.
        base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            device_map="cpu", 
            torch_dtype=torch.float32,
            low_cpu_mem_usage=True,
            offload_folder="offload_folder" # Usa disco se RAM acabar
        )
        
        model = PeftModel.from_pretrained(base_model, ADAPTER_REPO)
        model = model.merge_and_unload()
        
        container.empty()
        return tokenizer, model, "IA LOCAL (TinyLlama)"

    except Exception as e:
        # SE DER ERRO DE MEM√ìRIA, N√ÉO TRAVA O APP.
        container.warning("‚ö†Ô∏è Mem√≥ria do servidor cheia. Ativando MODO DE SEGURAN√áA (Demo/API).")
        print(f"Erro de carga: {e}")
        return None, None, "MODO DEMONSTRA√á√ÉO (Simulado)"

# Carrega o sistema
tokenizer, model, MODE_STATUS = load_engine_safely()

# --- 2. L√ìGICA DE RESPOSTA (H√çBRIDA) ---
def gerar_resposta(tipo_agente, dados_iot, prompt_usuario=None):
    """
    Gera a resposta. Se a IA local n√£o carregou, usa l√≥gica simulada inteligente.
    """
    
    # >>> CEN√ÅRIO 1: IA LOCAL EST√Å FUNCIONANDO
    if model and tokenizer:
        try:
            # Monta o prompt
            if tipo_agente == "tecnico":
                prompt_final = (
                    f"Com temperatura {dados_iot['Temperatura']}, umidade {dados_iot['Umidade']}, "
                    f"solo {dados_iot['Tipo_Solo']} para cultura {dados_iot['Cultura']}, "
                    f"N={dados_iot['N']}, P={dados_iot['P']}, K={dados_iot['K']}. "
                    f"Qual fertilizante usar?"
                )
                role = "T√©cnico Agr√≠cola Especialista"
            else:
                prompt_final = prompt_usuario
                role = "Assistente de Agroecologia"

            # Formato ChatML
            messages = [
                {"role": "system", "content": f"Voc√™ √© um {role}. Seja breve e t√©cnico."},
                {"role": "user", "content": prompt_final}
            ]
            
            input_ids = tokenizer.apply_chat_template(
                messages, add_generation_prompt=True, return_tensors="pt"
            )
            
            with torch.no_grad():
                outputs = model.generate(
                    input_ids, max_new_tokens=200, do_sample=True, temperature=0.4
                )
            
            return tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)
            
        except Exception:
            return "Erro na infer√™ncia. Mudando para backup."

    # >>> CEN√ÅRIO 2: MODO DEMONSTRA√á√ÉO (FALLBACK SE O SERVIDOR FALHAR)
    # Isso garante que sua apresenta√ß√£o NUNCA falhe
    time.sleep(2) # Simula tempo de pensamento
    
    if tipo_agente == "tecnico":
        # L√≥gica simples baseada nos dados para parecer real
        if dados_iot['P'] < 15:
            return "Recomenda√ß√£o: **NPK 14-35-14**\n\n**Motivo:** N√≠veis cr√≠ticos de F√≥sforo (P) detectados. Necess√°rio refor√ßo para desenvolvimento radicular."
        elif dados_iot['N'] < 20:
            return "Recomenda√ß√£o: **Ureia Agr√≠cola**\n\n**Motivo:** Baixo teor de Nitrog√™nio. Aplicar em cobertura para estimular crescimento vegetativo."
        else:
            return "Recomenda√ß√£o: **NPK 10-10-10**\n\n**Motivo:** Solo equilibrado, recomendada apenas aduba√ß√£o de manuten√ß√£o."
            
    elif tipo_agente == "ecologico":
        return (
            f"**Plano de Manejo Ecol√≥gico para {dados_iot['Cultura']} em Solo {dados_iot['Tipo_Solo']}:**\n\n"
            "1. **Cobertura Morta:** Essencial devido √† temperatura de " + str(dados_iot['Temperatura']) + "¬∞C para evitar evapora√ß√£o.\n"
            "2. **Aduba√ß√£o Verde:** Introduzir feij√£o-de-porco nas entrelinhas.\n"
            "3. **Biofertilizante:** Aplica√ß√£o foliar de Supermagro a cada 15 dias."
        )
    
    else: # Chatbot geral
        return "Como estou operando em modo de seguran√ßa (Demo), sugiro consultar um agr√¥nomo local para esta quest√£o espec√≠fica sobre pragas."

# --- 3. SIDEBAR IOT ---
st.sidebar.image("https://img.shields.io/badge/YBY.AI-System-green", use_container_width=True)
st.sidebar.caption(f"Status do Sistema: **{MODE_STATUS}**")

if 'iot' not in st.session_state:
    st.session_state['iot'] = {
        'Temperatura': 28.5, 'Umidade': 45.0, 'Solo_Umid': 30.0,
        'Tipo_Solo': 'Arenoso', 'Cultura': 'Milho',
        'N': 12, 'P': 8, 'K': 20
    }

if st.sidebar.button("üîÑ Ler Sensores"):
    st.session_state['iot'] = {
        'Temperatura': round(random.uniform(22, 38), 1),
        'Umidade': round(random.uniform(30, 80), 1),
        'Solo_Umid': round(random.uniform(10, 60), 1),
        'Tipo_Solo': random.choice(['Arenoso', 'Argiloso', 'Misto']),
        'Cultura': random.choice(['Milho', 'Feij√£o', 'Mandioca', 'Palma']),
        'N': random.randint(5, 60), 'P': random.randint(5, 60), 'K': random.randint(5, 60)
    }
    st.sidebar.success("Dados recebidos!")

d = st.session_state['iot']

# M√©tricas Visuais
c1, c2 = st.sidebar.columns(2)
c1.metric("üå°Ô∏è Temp", f"{d['Temperatura']}¬∞C")
c2.metric("üíß Solo", f"{d['Solo_Umid']}%", delta_color="inverse", delta="-Seco" if d['Solo_Umid'] < 30 else "Ok")
st.sidebar.info(f"Solo: **{d['Tipo_Solo']}** | Cultura: **{d['Cultura']}**")
st.sidebar.markdown("### Nutrientes (NPK)")
cc1, cc2, cc3 = st.sidebar.columns(3)
cc1.metric("N", d['N'])
cc2.metric("P", d['P'])
cc3.metric("K", d['K'])

# --- 4. TELA PRINCIPAL ---
st.title("üåµ YBY.AI: Intelig√™ncia do Semi√°rido")
st.markdown("Plataforma integrada de **IoT + IA Generativa** para agricultura de precis√£o.")

tab1, tab2 = st.tabs(["üìä An√°lise de Solo & Manejo", "üí¨ Chatbot Rural"])

# ABA 1: RELAT√ìRIOS
with tab1:
    st.subheader("Painel de Decis√£o Agron√¥mica")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### 1. Corre√ß√£o Qu√≠mica")
        st.caption("Analisa NPK para recomendar fertilizante mineral.")
        if st.button("üíä An√°lise Qu√≠mica (IA)", use_container_width=True):
            with st.spinner("Processando dados..."):
                res = gerar_resposta("tecnico", d)
                st.success("Recomenda√ß√£o Gerada:")
                st.markdown(res)
                
    with col2:
        st.markdown("### 2. Manejo Ecol√≥gico")
        st.caption("Estrat√©gias regenerativas e conviv√™ncia com a seca.")
        if st.button("üå≥ An√°lise Ecol√≥gica (IA)", use_container_width=True):
            with st.spinner("Consultando base agroecol√≥gica..."):
                res = gerar_resposta("ecologico", d)
                st.info("Plano de A√ß√£o:")
                st.markdown(res)

# ABA 2: CHAT
with tab2:
    st.subheader("Assistente Virtual")
    
    if "chat" not in st.session_state:
        st.session_state.chat = []
        
    for msg in st.session_state.chat:
        with st.chat_message(msg["role"]):
            st.write(msg["content"])
            
    if prompt := st.chat_input("D√∫vidas? (ex: Como plantar palma adensada?)"):
        st.session_state.chat.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)
            
        with st.chat_message("assistant"):
            with st.spinner("Digitando..."):
                resp = gerar_resposta("chat", d, prompt)
                st.write(resp)
                st.session_state.chat.append({"role": "assistant", "content": resp})
